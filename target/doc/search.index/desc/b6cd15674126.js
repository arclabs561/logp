rd_("AjContains the success valueCnCoefficient of determination for the linear fit in log-log \xe2\x80\xa6AhContains the error valuemf-DivergencesAoReturns the argument unchanged.00000BaCalls <code>U::from(self)</code>.00000CiEstimated log(C) in the same log base used internally \xe2\x80\xa6dsurpAfZipf / power-law tailsBnError types for information theory operations.BiEstimated exponent \xce\xb1 in f(r) ~ C / r^\xce\xb1.BcSublinear Estimation via the UnseenBdResult of a log-log linear Zipf fit.BlShannon entropy: H(p) = -\xce\xa3 p(x) log\xe2\x82\x82 p(x)BaNumber of points used in the fit.ClEncountered a non-finite intermediate (should not happen \xe2\x80\xa6BhErrors returned by Zipf fitting helpers.CbNumber of fingerprint entries to fit (default: 10)AeInput slice is empty.CcGeometric ratio for probability grid (default: 1.5)CjInternal consistency error: a step had no contributing \xe2\x80\xa6ChCompute the fingerprint of a sample: F[i] = count of \xe2\x80\xa6BhConfiguration for the unseen estimators.Blf-divergence with custom generator function.BkSlack factor for constraints (default: 1.0)BmCross-entropy: H(p, q) = -\xce\xa3 p(x) log\xe2\x82\x82 q(x)CgJensen-Shannon divergence: JS(p, q) = \xc2\xbdKL(p\xe2\x80\x96m) + \xe2\x80\xa6ChKL divergence: KL(p \xe2\x80\x96 q) = \xce\xa3 p(x) log\xe2\x82\x82(p(x) / q(x))ChTwo inputs were expected to have equal lengths (e.g. \xe2\x80\xa6C`Internal consistency error (mismatched lengths).BdCount distinct elements in a sample.CgEstimate entropy of a distribution including unseen \xe2\x80\xa6C`Estimate support size including unseen elements.BgNot enough usable points to fit a line.CaEstimate total variation distance between two \xe2\x80\xa6CkTotal variation distance: TV(P, Q) = (1/2) \xce\xa3 |p(x) - q(x)|BfA log-probability was NaN or infinite.Ci\xce\xb1-divergence: D_\xce\xb1(P || Q) = (1/(\xce\xb1(\xce\xb1-1))) (\xce\xa3 p^\xce\xb1 \xe2\x80\xa6ClR\xc3\xa9nyi divergence of order \xce\xb1: D_\xce\xb1(P || Q) = (1/(\xce\xb1-1)) \xe2\x80\xa6CcHellinger distance squared (the f-divergence form).CkRecover the underlying histogram from a sample fingerprint.CmCompute per-step EntCE (bits/token) from aligned per-step \xe2\x80\xa6CkHellinger distance: H(P, Q) = (1/\xe2\x88\x9a2) \xe2\x88\x9a\xce\xa3 (\xe2\x88\x9ap(x) - \xe2\x80\xa6CmMutual information from joint distribution: I(X;Y) = H(X) \xe2\x80\xa6BkConditional entropy: H(Y|X) = H(X,Y) - H(X)BiEntropy calibration for generative modelsCkEntropy calibration error: entropy - log loss (bits/token).BnFit a Zipf law exponent from raw token counts.CcSymmetric chi-squared divergence: (\xcf\x87\xc2\xb2(P||Q) + \xe2\x80\xa6ClCompute mean negative log-probability in bits/token from \xe2\x80\xa6BeBhattacharyya distance: -ln(BC(P, Q))CmChi-squared divergence: \xcf\x87\xc2\xb2(P || Q) = \xce\xa3 (p(x) - q(x))\xc2\xb2 \xe2\x80\xa6CnMean entropy over model generations, estimated by mean NLL \xe2\x80\xa6CiEstimate properties of a distribution with confidence \xe2\x80\xa6CbFisher information distance (approximation via \xe2\x80\xa6BlErrors for entropy calibration computations.ChSummary statistics for entropy calibration (bits per \xe2\x80\xa6C`Good-Turing estimate of unseen probability mass.CmMean log loss on reference text, estimated by mean NLL of \xe2\x80\xa6ClCompute entropy calibration stats in bits/token from two \xe2\x80\xa6CgBhattacharyya coefficient: BC(P, Q) = \xce\xa3 \xe2\x88\x9a(p(x) q(x))BmEntropy estimation with custom configuration.BmSupport estimation with custom configuration.CjTensorization of squared Hellinger distance for i.i.d. \xe2\x80\xa6CjCompute per-step mean NLL (bits/token) from a batch of \xe2\x80\xa6CnNormalized mutual information: NMI(X,Y) = 2*I(X;Y) / (H(X) \xe2\x80\xa6BmHistogram recovery with custom configuration.ClEstimate how much larger the training set needs to be to \xe2\x80\xa6CdCompute the singleton mass scaling exponent from \xe2\x80\xa6")