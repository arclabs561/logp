<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="surp"><title>surp - Rust</title><script>if(window.location.protocol!=="file:")document.head.insertAdjacentHTML("beforeend","SourceSerif4-Regular-6b053e98.ttf.woff2,FiraSans-Italic-81dc35de.woff2,FiraSans-Regular-0fe48ade.woff2,FiraSans-MediumItalic-ccf7e434.woff2,FiraSans-Medium-e1aa3f0a.woff2,SourceCodePro-Regular-8badfe75.ttf.woff2,SourceCodePro-Semibold-aa29a496.ttf.woff2".split(",").map(f=>`<link rel="preload" as="font" type="font/woff2"href="../static.files/${f}">`).join(""))</script><link rel="stylesheet" href="../static.files/normalize-9960930a.css"><link rel="stylesheet" href="../static.files/rustdoc-e56847b5.css"><meta name="rustdoc-vars" data-root-path="../" data-static-root-path="../static.files/" data-current-crate="surp" data-themes="" data-resource-suffix="" data-rustdoc-version="1.91.1 (ed61e7d7e 2025-11-07)" data-channel="1.91.1" data-search-js="search-e256b49e.js" data-stringdex-js="stringdex-c3e638e9.js" data-settings-js="settings-c38705f0.js" ><script src="../static.files/storage-e2aeef58.js"></script><script defer src="../crates.js"></script><script defer src="../static.files/main-6dc2a7f3.js"></script><noscript><link rel="stylesheet" href="../static.files/noscript-263c88ec.css"></noscript><link rel="alternate icon" type="image/png" href="../static.files/favicon-32x32-eab170b8.png"><link rel="icon" type="image/svg+xml" href="../static.files/favicon-044be391.svg"></head><body class="rustdoc mod crate"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><rustdoc-topbar><h2><a href="#">Crate surp</a></h2></rustdoc-topbar><nav class="sidebar"><div class="sidebar-crate"><h2><a href="../surp/index.html">surp</a><span class="version">0.1.0</span></h2></div><div class="sidebar-elems"><ul class="block"><li><a id="all-types" href="all.html">All Items</a></li></ul><section id="rustdoc-toc"><h3><a href="#">Sections</a></h3><ul class="block top-toc"><li><a href="#surp" title="surp">surp</a><ul><li><a href="#key-functions" title="Key Functions">Key Functions</a></li><li><a href="#quick-start" title="Quick Start">Quick Start</a></li><li><a href="#why-these-matter-for-ml" title="Why These Matter for ML">Why These Matter for ML</a></li><li><a href="#connections" title="Connections">Connections</a></li><li><a href="#references" title="References">References</a></li><li><a href="#sublinear-estimation" title="Sublinear Estimation">Sublinear Estimation</a></li><li><a href="#what-can-go-wrong" title="What Can Go Wrong">What Can Go Wrong</a></li></ul></li></ul><h3><a href="#reexports">Crate Items</a></h3><ul class="block"><li><a href="#reexports" title="Re-exports">Re-exports</a></li><li><a href="#modules" title="Modules">Modules</a></li><li><a href="#enums" title="Enums">Enums</a></li><li><a href="#functions" title="Functions">Functions</a></li><li><a href="#types" title="Type Aliases">Type Aliases</a></li></ul></section><div id="rustdoc-modnav"></div></div></nav><div class="sidebar-resizer" title="Drag to resize sidebar"></div><main><div class="width-limiter"><section id="main-content" class="content"><div class="main-heading"><h1>Crate <span>surp</span>&nbsp;<button id="copy-path" title="Copy item path to clipboard">Copy item path</button></h1><rustdoc-toolbar></rustdoc-toolbar><span class="sub-heading"><a class="src" href="../src/surp/lib.rs.html#1-578">Source</a> </span></div><details class="toggle top-doc" open><summary class="hideme"><span>Expand description</span></summary><div class="docblock"><h2 id="surp"><a class="doc-anchor" href="#surp">§</a>surp</h2>
<p>Information theory primitives: surprisal, entropy, KL divergence.</p>
<p>(surp: from “surprisal”, the fundamental unit of information) If an event
is certain, learning it happened conveys no information. If it’s rare,
learning it happened is highly informative.</p>
<h3 id="key-functions"><a class="doc-anchor" href="#key-functions">§</a>Key Functions</h3><div><table><thead><tr><th>Function</th><th>Measures</th><th>Formula</th></tr></thead><tbody>
<tr><td><a href="fn.entropy.html" title="fn surp::entropy"><code>entropy</code></a></td><td>Uncertainty in distribution</td><td>H(p) = -Σ p(x) log p(x)</td></tr>
<tr><td><a href="fn.cross_entropy.html" title="fn surp::cross_entropy"><code>cross_entropy</code></a></td><td>Expected bits using wrong code</td><td>H(p,q) = -Σ p(x) log q(x)</td></tr>
<tr><td><a href="fn.kl_divergence.html" title="fn surp::kl_divergence"><code>kl_divergence</code></a></td><td>“Distance” from q to p</td><td>KL(p‖q) = Σ p(x) log(p(x)/q(x))</td></tr>
<tr><td><a href="fn.js_divergence.html" title="fn surp::js_divergence"><code>js_divergence</code></a></td><td>Symmetric “distance”</td><td>JS(p,q) = ½KL(p‖m) + ½KL(q‖m)</td></tr>
<tr><td><a href="fn.mutual_information.html" title="fn surp::mutual_information"><code>mutual_information</code></a></td><td>Shared information</td><td>I(X;Y) = H(X) + H(Y) - H(X,Y)</td></tr>
</tbody></table>
</div><h3 id="quick-start"><a class="doc-anchor" href="#quick-start">§</a>Quick Start</h3>
<div class="example-wrap"><pre class="rust rust-example-rendered"><code><span class="kw">use </span>surp::{entropy, kl_divergence, js_divergence};

<span class="kw">let </span>p = [<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>];  <span class="comment">// uniform
</span><span class="kw">let </span>q = [<span class="number">0.5</span>, <span class="number">0.25</span>, <span class="number">0.125</span>, <span class="number">0.125</span>]; <span class="comment">// skewed

</span><span class="kw">let </span>h = entropy(<span class="kw-2">&amp;</span>p);           <span class="comment">// 2.0 bits (maximum for 4 outcomes)
</span><span class="kw">let </span>kl = kl_divergence(<span class="kw-2">&amp;</span>p, <span class="kw-2">&amp;</span>q); <span class="comment">// asymmetric divergence
</span><span class="kw">let </span>js = js_divergence(<span class="kw-2">&amp;</span>p, <span class="kw-2">&amp;</span>q); <span class="comment">// symmetric, bounded [0, 1]</span></code></pre></div><h3 id="why-these-matter-for-ml"><a class="doc-anchor" href="#why-these-matter-for-ml">§</a>Why These Matter for ML</h3>
<ul>
<li><strong>Cross-entropy loss</strong>: Standard classification loss is cross-entropy
between true labels (one-hot) and predicted probabilities</li>
<li><strong>KL divergence</strong>: VAE latent space regularization, knowledge distillation</li>
<li><strong>Mutual information</strong>: Feature selection, representation learning bounds</li>
<li><strong>JS divergence</strong>: GAN training (original formulation)</li>
</ul>
<h3 id="connections"><a class="doc-anchor" href="#connections">§</a>Connections</h3>
<ul>
<li><a href="../kern"><code>kern</code></a>: MMD and KL both measure distribution “distance”</li>
<li><a href="../wass"><code>wass</code></a>: Wasserstein vs entropy-based divergences</li>
<li><a href="../stratify"><code>stratify</code></a>: NMI for cluster evaluation uses this crate</li>
<li><a href="../fynch"><code>fynch</code></a>: Temperature scaling affects entropy calibration</li>
</ul>
<h3 id="references"><a class="doc-anchor" href="#references">§</a>References</h3>
<ul>
<li>Shannon (1948). “A Mathematical Theory of Communication”</li>
<li>Cover &amp; Thomas (2006). “Elements of Information Theory”</li>
</ul>
<h3 id="sublinear-estimation"><a class="doc-anchor" href="#sublinear-estimation">§</a>Sublinear Estimation</h3>
<p>The <a href="unseen/index.html" title="mod surp::unseen"><code>unseen</code></a> module implements the Valiant-Valiant estimators
for entropy and support size from sublinear samples.</p>
<p><strong>Key insight</strong>: Given O(k/log k) samples from a distribution over k elements,
we can accurately estimate entropy, support size, and distances. This is
optimal up to constant factors.</p>
<p><strong>References</strong>:</p>
<ul>
<li>Valiant &amp; Valiant (2011). “Estimating the Unseen: An n/log(n)-Sample Estimator”</li>
<li>Valiant &amp; Valiant (2017). “Estimating the Unseen: Improved Estimators” (JACM)</li>
</ul>
<h3 id="what-can-go-wrong"><a class="doc-anchor" href="#what-can-go-wrong">§</a>What Can Go Wrong</h3>
<ol>
<li><strong>Zero probabilities in KL</strong>: KL(p||q) is infinite if q(x)=0 where p(x)&gt;0.
Use JS divergence or add smoothing.</li>
<li><strong>Not normalized</strong>: Many functions assume probabilities sum to 1. Check inputs.</li>
<li><strong>Entropy of continuous distributions</strong>: Discrete entropy ≠ differential entropy.
Discretization bin width affects results.</li>
<li><strong>Sample size too small</strong>: Entropy estimators are biased for small samples.
Miller-Madow correction or Valiant-Valiant for sublinear regime.</li>
<li><strong>Floating point issues</strong>: Very small probabilities can cause log(0) = -inf.</li>
</ol>
</div></details><h2 id="reexports" class="section-header">Re-exports<a href="#reexports" class="anchor">§</a></h2><dl class="item-table reexports"><dt id="reexport.alpha_divergence"><code>pub use fdiv::<a class="fn" href="fdiv/fn.alpha_divergence.html" title="fn surp::fdiv::alpha_divergence">alpha_divergence</a>;</code></dt><dt id="reexport.bhattacharyya_coefficient"><code>pub use fdiv::<a class="fn" href="fdiv/fn.bhattacharyya_coefficient.html" title="fn surp::fdiv::bhattacharyya_coefficient">bhattacharyya_coefficient</a>;</code></dt><dt id="reexport.bhattacharyya_distance"><code>pub use fdiv::<a class="fn" href="fdiv/fn.bhattacharyya_distance.html" title="fn surp::fdiv::bhattacharyya_distance">bhattacharyya_distance</a>;</code></dt><dt id="reexport.chi_squared_divergence"><code>pub use fdiv::<a class="fn" href="fdiv/fn.chi_squared_divergence.html" title="fn surp::fdiv::chi_squared_divergence">chi_squared_divergence</a>;</code></dt><dt id="reexport.f_divergence"><code>pub use fdiv::<a class="fn" href="fdiv/fn.f_divergence.html" title="fn surp::fdiv::f_divergence">f_divergence</a>;</code></dt><dt id="reexport.hellinger_distance"><code>pub use fdiv::<a class="fn" href="fdiv/fn.hellinger_distance.html" title="fn surp::fdiv::hellinger_distance">hellinger_distance</a>;</code></dt><dt id="reexport.hellinger_squared"><code>pub use fdiv::<a class="fn" href="fdiv/fn.hellinger_squared.html" title="fn surp::fdiv::hellinger_squared">hellinger_squared</a>;</code></dt><dt id="reexport.hellinger_squared_tensorized"><code>pub use fdiv::<a class="fn" href="fdiv/fn.hellinger_squared_tensorized.html" title="fn surp::fdiv::hellinger_squared_tensorized">hellinger_squared_tensorized</a>;</code></dt><dt id="reexport.renyi_divergence"><code>pub use fdiv::<a class="fn" href="fdiv/fn.renyi_divergence.html" title="fn surp::fdiv::renyi_divergence">renyi_divergence</a>;</code></dt><dt id="reexport.total_variation"><code>pub use fdiv::<a class="fn" href="fdiv/fn.total_variation.html" title="fn surp::fdiv::total_variation">total_variation</a>;</code></dt></dl><h2 id="modules" class="section-header">Modules<a href="#modules" class="anchor">§</a></h2><dl class="item-table"><dt><a class="mod" href="entropy_calibration/index.html" title="mod surp::entropy_calibration">entropy_<wbr>calibration</a></dt><dd>Entropy calibration for generative models</dd><dt><a class="mod" href="fdiv/index.html" title="mod surp::fdiv">fdiv</a></dt><dd>f-Divergences</dd><dt><a class="mod" href="unseen/index.html" title="mod surp::unseen">unseen</a></dt><dd>Sublinear Estimation via the Unseen</dd><dt><a class="mod" href="zipf/index.html" title="mod surp::zipf">zipf</a></dt><dd>Zipf / power-law tails</dd></dl><h2 id="enums" class="section-header">Enums<a href="#enums" class="anchor">§</a></h2><dl class="item-table"><dt><a class="enum" href="enum.Error.html" title="enum surp::Error">Error</a></dt><dd>Error types for information theory operations.</dd></dl><h2 id="functions" class="section-header">Functions<a href="#functions" class="anchor">§</a></h2><dl class="item-table"><dt><a class="fn" href="fn.conditional_entropy.html" title="fn surp::conditional_entropy">conditional_<wbr>entropy</a></dt><dd>Conditional entropy: H(Y|X) = H(X,Y) - H(X)</dd><dt><a class="fn" href="fn.cross_entropy.html" title="fn surp::cross_entropy">cross_<wbr>entropy</a></dt><dd>Cross-entropy: H(p, q) = -Σ p(x) log₂ q(x)</dd><dt><a class="fn" href="fn.distinct_count.html" title="fn surp::distinct_count">distinct_<wbr>count</a></dt><dd>Count distinct elements in a sample.</dd><dt><a class="fn" href="fn.entropy.html" title="fn surp::entropy">entropy</a></dt><dd>Shannon entropy: H(p) = -Σ p(x) log₂ p(x)</dd><dt><a class="fn" href="fn.fingerprint.html" title="fn surp::fingerprint">fingerprint</a></dt><dd>Compute the fingerprint of a sample: F[i] = count of elements appearing exactly i times.</dd><dt><a class="fn" href="fn.good_turing_unseen_mass.html" title="fn surp::good_turing_unseen_mass">good_<wbr>turing_<wbr>unseen_<wbr>mass</a></dt><dd>Good-Turing estimate of unseen probability mass.</dd><dt><a class="fn" href="fn.js_divergence.html" title="fn surp::js_divergence">js_<wbr>divergence</a></dt><dd>Jensen-Shannon divergence: JS(p, q) = ½KL(p‖m) + ½KL(q‖m)</dd><dt><a class="fn" href="fn.kl_divergence.html" title="fn surp::kl_divergence">kl_<wbr>divergence</a></dt><dd>KL divergence: KL(p ‖ q) = Σ p(x) log₂(p(x) / q(x))</dd><dt><a class="fn" href="fn.mutual_information.html" title="fn surp::mutual_information">mutual_<wbr>information</a></dt><dd>Mutual information from joint distribution: I(X;Y) = H(X) + H(Y) - H(X,Y)</dd><dt><a class="fn" href="fn.normalized_mutual_information.html" title="fn surp::normalized_mutual_information">normalized_<wbr>mutual_<wbr>information</a></dt><dd>Normalized mutual information: NMI(X,Y) = 2*I(X;Y) / (H(X) + H(Y))</dd></dl><h2 id="types" class="section-header">Type Aliases<a href="#types" class="anchor">§</a></h2><dl class="item-table"><dt><a class="type" href="type.Result.html" title="type surp::Result">Result</a></dt></dl></section></div></main></body></html>